\documentclass[12pt,a4paper]{article}
\usepackage[spanish,es-tabla]{babel}

\usepackage[utf8]{inputenc} % Escribir con acentos, ~n...
\usepackage{eurosym} % s´ımbolo del euro
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height
\usepackage{listings}             % Incluye el paquete listing
\usepackage[cache=false]{minted}
\usepackage{graphics,graphicx, float} %para incluir imágenes y colocarlas
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}
\usepackage{multirow}
\usepackage{array}
\usepackage{diagbox}

\title{
\normalfont \normalsize 
\textsc{{\bf Aprendizaje Automático (2018-2019)} \\ Grado en Ingeniería Informática \\ Universidad de Granada} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Práctica 1 \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
\includegraphics{images/logo.png}	
}

\author{Antonio Jesús Heredia Castillo} % Nombre y apellidos

\date{\normalsize\today} % Incluye la fecha actual

%----------------------------------------------------------------------------------------
% DOCUMENTO
%----------------------------------------------------------------------------------------

\begin{document}

\maketitle % Muestra el Título
\newpage %inserta un salto de página
\tableofcontents % para generar el índice de contenidos
\listoffigures
\listoftables
\newpage

\section{Gradiente descendente}
\subsection{Implementar el algoritmo de gradiente descendente}
He querido implementar el algoritmo del gradiente descendente de la forma mas general que he podido. Para ello, he creado una función que lo calcula. Esta función recibe  los siguientes parámetros:
\begin{itemize}
	\item lr (float): Tasa de aprendizaje
	\item punto (array[x,y]): Punto desde el cual se quiere aplicar el algoritmo
	\item función (float): Función a la que se le quiere aplicar el gradiente descendente
	\item iteraciones (int): Numero máximo de iteraciones que aplicara el algoritmo
\end{itemize}
Esta subrutina se encarga de hacer las derivadas parciales de la función y de calcular de forma iterativa el punto mínimo de la función. En esta primera versión del algoritmo, la única condición de parada va a ser el numero de iteraciones que le pasa el usuario. Podemos ver el código del algoritmo aquí.
\begin{minted}[mathescape,
	frame=lines,
	framesep=2mm]{python}
_iter = 0 #Contador de iteraciones
_dx = sp.diff(funcion, x) #Derivadas parciales
_dy = sp.diff(funcion, y)
#Lambdifyco las derivadas parciales para calcular mas rapido el valor
_lam_dx = sp.lambdify((x,y),_dx) 
_lam_dy = sp.lambdify((x,y),_dy)
while True:
	#creo una copia del punto, para que a la hora de evuluar el x, 
	#no lo pierda para y
	_punto_copia = np.copy(_punto)
	#si hay mas iteraicones de las que yo quiero paro
	if _iter > _iteraciones:
		break
	#calculo el gradiente
	_punto[0] = _punto[0]-_lr*_lam_dx(_punto_copia[0],_punto_copia[1])
	_punto[1] = _punto[1]-_lr*_lam_dy(_punto_copia[0],_punto_copia[1])
	_iter =_iter+1
return _punto   
\end{minted}

\subsection{Considerar la Función \(  E(u, v) = (u^2e^v - 2v^2e^{-u})^2\). Usar gradiente descendente para encontrar un mínimo de esta función, comenzando desde el punto \((u, v) = (1, 1)\) y usando una tasa de aprendizaje \(\eta = 0,01\).}
\subsubsection{Calcular analíticamente y mostrar la expresión del gradiente de la función \(E(u, v)\)}
Lo primero que tenemos que saber es que para calcular el gradiente, necesitamos las derivadas parciales de \(E(u,v)\). \\
La derivada respecto de \(u\) seria: 

\begin{equation}
\frac{\partial E(u,v)}
{\partial u} = 2\left(u^2e^v-2v^2e^{-u}\right)\left(2e^vu+2v^2e^{-u}\right)
\end{equation}
La derivada respecto de \(v\) seria: 
\begin{equation}
\frac{\partial E(u,v)}
{\partial v} = 2\left(u^2e^v-2v^2e^{-u}\right)\left(u^2e^v-4e^{-u}v\right)
\end{equation}
Ahora con las derivadas, ya podemos obtener el gradiente. Los nuevos valores de \(u\) y \(v\) serán:
\begin{equation}
u' = u - 2\eta\left(u^2e^v-2v^2e^{-u}\right)\left(2e^vu+2v^2e^{-u}\right)
\end{equation}

\begin{equation}
v' = v - 2\eta\left(u^2e^v-2v^2e^{-u}\right)\left(u^2e^v-4e^{-u}v\right)
\end{equation}

\subsubsection{¿Cuántas iteraciones tarda el algoritmo en obtener por primera vez un valor de \(E(u, v)\) inferior a \(10^{-14}\) . (Usar flotantes de 64 bits)}
Para realizar este ejercicio, solo he metido una condición de parada en el algoritmo. La condición de parada en este caso es que el valor de evaluar \(E(u,v)\) en un determinada iteración sea \(E(u,v)<10^{-14}\)

\begin{minted}[mathescape,
frame=lines,
framesep=2mm]{python} 
if _iter > _iteraciones or 
   _lam_f(_punto_copia[0],_punto_copia[1]) < np.float64(10**-14):
	print ("La funcion ha acabado a las " 
	+ str(_iter) + " iteraciones")
	break
\end{minted}
En este caso el numero de iteraciones necesarias ha sido \textbf{33}, como se puede ver en la Figura\ref{figura1}.

\begin{figure}[H]  %con el [H] le obligamos a situar aquí la figura
\centering
\includegraphics{images/primerDescensoGradiente.png}  %el parámetro scale permite agrandar o achicar la imagen. En el nombre de archivo puede especificar directorios
\caption{Representación de como avanza el gradiente en la función \(E(u,v)\). Punto en amarillo inicio y punto final en blanco.}
\label{figura1}
\end{figure}

\subsubsection{¿En qué coordenadas \((u, v)\) se alcanzó por primera vez un valor igual o menor a \(10^{-14}\) en el apartado anterior.}
Como el algoritmo del gradiente me devuelve el punto en el que acaba, lo único que tengo que hacer es mostrarlo por pantalla. En este caso el punto en el que se alcanza \(10^{-14}\) es \textbf{\((0.61920768,0.96844827)\)}.

\subsection{Considerar ahora la función \(  f(x,y)=x^2+2y^2+2\sin(2\pi x)\sin(2\pi y)\)}
\subsubsection{Usar gradiente descendente para minimizar esta función. Usar como punto inicial \((x_{0}=0.1, y_{0}=0.1)\), (tasa de aprendizaje \(\eta=0,01\) y un máximo de 50 iteraciones. Generar un gráfico de cómo desciende el valor de la función con las iteraciones. Repetir el experimento pero usando \(\eta=0.11\), comentar las diferencias y su dependencia de \(\eta\).}
Usando un \(\eta=0.01\), podemos gráficamente Figura  \ref{figura2} como el gradiente desciende de forma adecuada. En este caso acaba en una altura de \(-1.8200785415471563\)

\begin{figure}[H]  %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics{images/segundoDescensoGradiente1.png}  
	\caption{Gradiente de la función \(F(u,v)\) con   \((x_{0}=0.1, y_{0}=0.1)\) y \(\eta=0.01\). Punto en amarillo inicio y punto final en blanco.}
	\label{figura2}
\end{figure}
En cambio cuando usamos \(\eta=0.1\) , podemos ver (Figura  \ref{figura3}) como no funciona bien. En este caso acaba en una altura de \(2.5007003656467446\), que es incluso mas alto que desde donde empezamos (\(0.7209830056250534\)).
\begin{figure}[H]  %con el [H] le obligamos a situar aquí la figura
	\centering
	\includegraphics{images/segundoDescensoGradiente2.png}  
	\caption{Gradiente de la función \(F(u,v)\) con   \((x_{0}=0.1, y_{0}=0.1)\) y \(\eta=0.1\). Punto en amarillo inicio y punto final en blanco.}
	\label{figura3}
\end{figure}
Las diferencias entre usar un \(\eta\) muy grande o uno adecuado es clara viendo las imágenes, con \(\eta\) adecuado se encuentra ``rápidamente'' el mínimo. Con el \(\eta\) muy grande, el algoritmo da saltos muy grandes y por lo tanto no llega a encontrar el mínimo debido a que de una iteración a otra lo excede y empieza a rebotar sin encontrar nada.
\subsubsection{Obtener el valor mínimo y los valores de las variables \((x, y)\) en donde se alcanzan cuando el punto de inicio se fija: \((0,1, 0,1), (1, 1),(-0,5, -0,5),(-1, -1)\). Generar una tabla con los valores obtenidos}
Los datos obtenidos podemos verlos en la Tabla \ref{tab:title}. En este caso, no he dejado el calculo de todos los puntos en el código para no sobrecargar la salida por pantalla con datos y que el tiempo de ejecución no fuera muy alto. El campo ``valor'' indica la altura que alcanza la función en el punto \((x,y)\).
\begin{table}[H]

	\caption {} \label{tab:title} 
	   
	\begin{tabular}{|l|r|r|r|r|}
		\hline
		\backslashbox{Valor final}{Punto} &  \((0.1 , 0.1)\) & \((1 , 1)\) & \((-0.5 , -0.5)\) & \((-1 , -1)\)\\
		\hline
		x & 0.24380497 & 1.2180703  & -0.73137746 Km & -1.2180703\\
		\hline
		y & -0.23792582 & 0.71281195 & -0.23785536 & -0.71281195\\
		\hline
		Valor mínimo & -1.82007854 & 0.59326937 & -1.33248106 & 0.59326937\\
		\hline
	\end{tabular}
\end{table}
\subsubsection{¿Cuál sería su conclusión sobre la verdadera dificultad de encontrar el mínimo	global de una función arbitraria?}
Teniendo en cuenta los datos obtenidos en los apartados anteriores y como afecta tanto la tasa de aprendizaje como el punto donde se inicia el algoritmo. Llego a la conclusión que la verdadera dificultad radica en la elección de estos dos parámetros. Por ejemplo al elegir un ``mal'' punto de inicio puede no encontrar el punto mínimo absoluto de la función, si no un punto mínimo relativo a la posición desde donde ha iniciado. \\\\
Elegir una tasa de aprendizaje que no sea adecuada también nos va a dar bastantes problemas. Esto se debe a que si es muy grande, el algoritmo nunca va a llegar a converger debido a que va a ir rebotando de un lado al otro del mínimo, pero sin llegar a acercarse nunca. Si lo elegimos muy pequeño, el algoritmo daría pasos muy pequeños y tardaría demasiado en llegar al mínimo.\\\\
Por lo tanto desde mi punto de vista una vez que tienes todos los algoritmos implementados, la dificultad radica en elegir buenos valores para conseguir que nuestro algoritmo funcione de forma adecuada.
\end{document}