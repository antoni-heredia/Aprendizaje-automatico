\documentclass[12pt,a4paper]{article}

\usepackage[spanish,es-tabla]{babel}
\usepackage[a4paper,bindingoffset=0.1in,%
left=0.75in,right=0.75in,top=1in,bottom=1in,%
footskip=.2in]{geometry}
\usepackage[utf8]{inputenc} % Escribir con acentos, ~n...
\usepackage{eurosym} % s´ımbolo del euro
\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height
\usepackage{listings}             % Incluye el paquete listing
\usepackage[cache=false]{minted}
\usepackage{graphics,graphicx, float} %para incluir imágenes y colocarlas
\usepackage{hyperref}
\hypersetup{
	colorlinks,
	citecolor=black,
	filecolor=black,
	linkcolor=black,
	urlcolor=black
}
\usepackage{multirow}
\usepackage{array}
\usepackage{diagbox}
\usepackage{amsmath}
\usepackage{verbatim}
\begin{document}

\title{Aprendizaje autom\'atico. Cuestionario 1}

\author{
  Antonio Jesús Heredia Castillo\\
  \texttt{00000000A}
}

\date{}
\maketitle
\horrule{2pt}

\begin{enumerate}
	\item Identificar, para cada una de las siguientes tareas, cual es el prioblema, que tipo de	aprendizaje es el adecuado (supervisado, no supervisado, por refuerzo) y los elementos de aprendizaje (X , f, Ym ) que deberíamos usar en cada caso. Si una tarea se ajusta a más de un tipo, explicar como y describir los elementos para cada tipo.
	\begin{enumerate}
		\item  Clasificación automática de cartas por distrito postal.\\\\
		\textbf{Respuesta:}\\
		Esto es un caso típico de aprendizaje supervisado. En este caso se creara un modelo capaz de reconocer los diferentes números y así poder saber el distrito postal al que pertenece la carta. En este caso los valores de entrada serán datos de la imagen del dígito. Estos valores puede ser como hemos visto en practicas la intensidad promedio y la simetría de la misma. Por otro lado la salida seria cual es el dígito que estamos clasificando. 
		 
		\item Decidir si un determinado índice del mercado de valores subirá o bajará dentro de un periodo de tiempo fijado. \\\\
		\textbf{Respuesta:}\\
		Aunque a priori puede parecer fácilmente abordable por aprendizaje supervisado, creo que seria mas conveniente abordarlo por aprendizaje reforzado. Ya que en la bolsa influye demasiadas variables y seria difícil tenerlas todas en cuenta por nosotros. En cambio para un modelo de aprendizaje por refuerzo seria mas fácil encontrar los patrones que se dan cuando recibe un ``refuerzo'' positivo o negativo(que suba o baje el valor) .
		\item Hacer que un dron sea capaz de rodear un obstáculo\\\\		\textbf{Respuesta:}\\
		En este caso usaría aprendizaje por refuerzo. Usaría un simulador de drones en una computadora, para que el dron real no tuviera daños. La ``recompensa'' recibida seria superar el obstáculo. De esta forma no necesitaríamos ningún conjunto de datos anterior para poder realizar el entrenamiento del dron.
		
		\item Dada una colección de fotos de perros, posiblemente de distintas razas, establecer cuantas razas distintas hay representadas en la colección.\\\\		\textbf{Respuesta:}\\
		Para este supuesto intentaría utilizar aprendizaje no supervisado ya que no nos importa  que raza tiene cada perro, si no la cantidad de razas. De esta forma la maquina se encargara de encontrar semejanzas entre las diferentes fotografiás y las agrupara de manera natural según el perro que aparece, evitando tener datos etiquetados.
	\end{enumerate}


	\item ¿Cuales de los siguientes problemas son más adecuados para una aproximación por aprendizaje y cuales más adecuados para una aproximación por diseño? Justificar la decisión
	\begin{enumerate}
		\item Determinar si un vertebrado es mamífero, reptil, ave, anfibio o pez.\\\\		\textbf{Respuesta:}\\
		En este acaso usaría aproximación por diseño, ya que cada clase de vertebrados tienen características comunes que la diferencia de otras clases. Estas caracterizaras de cada clase son ya muy conocidas (por ejemplo, los mamíferos tienen glándulas mamarias) y seria fácil crear un algoritmo que lo resolviera.
		\item Determinar si se debe aplicar una campaña de vacunación contra una enfermedad.\\\\		\textbf{Respuesta:}\\
		Este problema intentaría abordarlo con una aproximación por diseño. Los virus pueden cambiar de un año para otro e incluso aparecer enfermedades nuevas o que no tengamos suficientes datos estadísticos como para entrenar a un modelo de aprendizaje. Por tanto usando el conocimiento de un infectologo podríamos crear un sistema algorítmico que prediga cuando seria necesario realizar  una campaña de evacuación. 
		\item Determinar perfiles de consumidor en una cadena de supermercados.\\\\		\textbf{Respuesta:}\\
		Aquí, al tener que hacer ``grupos'' de consumidores, podríamos usar una aproximación por aprendizaje. Este se podría encargar de buscar semejanzas entre los diferentes consumidores y agrupar los que mas se parezcan entre si.
		\item Determinar el estado anímico de una persona a partir de una foto de su cara.\\\\		\textbf{Respuesta:}\\
		También elegiría una aproximación por aprendizaje ya que, podemos proveer con un gran conjunto de datos etiquetados al sistema y que este se encargara de analizar las diferentes variables  que tuviera las imágenes proporcionadas y poder predecir con una imagen nueva el estado animico de la persona que aparece. 
		\item Determinar el ciclo óptimo para las luces de los semáforos en un cruce con mucho tráfico.\\\\		\textbf{Respuesta:}\\
		Esto seria un problema típico para resolver con aproximación por diseño. Sabemos bien como afecta las distintas variables al problema (si hay coches pasando, si  hay alguien esperando para cruzar la calle, etc), con estos datos seria fácil ajustar un algoritmo que satisfaga las necesidades de los usuarios.
	\end{enumerate}
	\item Construir un problema de aprendizaje desde datos para un problema de clasificación de fruta en una explotación agraria que produce mangos, papayas y guayabas. Identificar los siguientes elementos formales X , Y, D, f del problema. Dar una descripción de los mismos que pueda ser usada por un computador. ¿Considera que en este problema estamos ante un caso de etiquetas con ruido o sin ruido? Justificar las respuestas.\\\\
		\textbf{Respuesta:}\\
		\begin{enumerate}
			\item \textbf{X} : las variables $\{x_1,x_2,...,x_n\}$ con las que podemos definir las distintas frutas. En este caso puede ser color, tamaño, simetría, etc. 
			\item \textbf{Y}: Al ser un problema de clasificación discreto he elegido esta representación. $\mathcal{Y} = y \in \{mango, papaya, guayaba\}$
			\item El conjunto de datos seria $D = \{(x_1,y_1),...,(x_N, y_N) y_i=f(x_i),i=1,2,3,...,N/x_i \in \mathcal{X},y_i \in \mathcal{Y}\} $ 
			\item La función $f$ es desconocida y es la que intentamos buscar a partir del aprendizaje de los datos. $f : \mathcal{X} \mapsto \mathcal{Y}$
		\end{enumerate}
	\item Suponga una matriz cuadrada $A$ que admita la descomposición $A = X^TX$ para alguna matriz $X$ de números reales. Establezca una relación entre los valores singulares de las matriz $A$ y los valores singulares de $X$.\\\\\textbf{Respuesta: }\\
	Consideraremos la SVD de $X$ como $X=UDV^T$, ademas sabemos según las diapositivas (pagina 15/Sesión 2 Modelos lineales), $X^TX = UDDV^T$. Por lo tanto como tenemos que $A=X^TX = UDDV^T$, los valores singulares de la matriz $A$ serán el cuadrado de los valores singulares de los de $X$, ya que $D$ es una matriz diagonal de $n \times n$ y la multiplicación de D por ella misma es el cuadrado de cada valor de su diagonal. Esto es una propiedad de la matrices diagonales.
	\item Sean $x$ e $y$ dos vectores de características de dimensión $M \times 1$. La expresión $\operatorname{cov}(\mathbf{x}, \mathbf{y})=\frac{1}{M} \sum_{i=1}^{M}\left(x_{i}-\overline{x}\right)\left(y_{i}-\overline{y}\right)$ define la covarianza entre dichos vectores, donde $\overline{z}$ representa el valor medio de los elementos de $\textbf{z}$. Considere ahora una matriz $X$ cuyas columnas representan vectores de características. La matriz de covarianzas asociada a la matriz $X = (x_1 , x_2 , · · · , x_N )$ es el conjunto de covarianzas definidas por cada dos de sus vectores columnas. Es decir, $$\operatorname{cov}(\mathrm{X})=\left( \begin{array}{cccc}{\operatorname{cov}\left(\mathbf{x}_{1}, \mathbf{x}_{1}\right)} & {\operatorname{cov}\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)} & {\dots} & {\operatorname{cov}\left(\mathbf{x}_{1}, \mathbf{x}_{N}\right)} \\ {\operatorname{cov}\left(\mathbf{x}_{2}, \mathbf{x}_{1}\right)} & {\operatorname{cov}\left(\mathbf{x}_{2}, \mathbf{x}_{2}\right)} & {\cdots} & {\operatorname{cov}\left(\mathbf{x}_{2}, \mathbf{x}_{N}\right)} \\ {\cdots} & {\cdots} & {\cdots} & {\cdots} \\ {\operatorname{cov}\left(\mathbf{x}_{N}, \mathbf{x}_{1}\right)} & {\operatorname{cov}\left(\mathbf{x}_{N}, \mathbf{x}_{2}\right)} & {\cdots} & {\operatorname{cov}\left(\mathbf{x}_{N}, \mathbf{x}_{N}\right)}\end{array}\right)$$ Sea $1_{M}^{T}=(1,1, \cdots, 1)$ un vector $M \time 1$ de unos. Mostrar que representan las siguientes:
	expresiones
	\begin{enumerate}
		\item  $E 1=11^{T} \mathrm{X}$\\\\
		\textbf{Respuesta:}\\


\begin{equation}
\begin{split}
\begin{aligned}
\notag
	E_1 &= \begin{pmatrix}1_1\\1_2\\\cdots\\1_m\end{pmatrix} \times \begin{pmatrix} 1_1&1_2&&\cdots&&1_m \end{pmatrix} \times \begin{pmatrix}	
x_{11}&x_{21}&\cdots&x_{n1}\\ 
x_{12}&x_{22}&\cdots&x_{n2}\\ 
\cdots & \cdots & \cdots & \cdots \\
x_{1m}&x_{2m}&\cdots&x_{nm}
\end{pmatrix} \\ & = \begin{pmatrix}	
		1_{11}&1_{21}&\cdots&1_{m1}\\ 
		1_{12}&1_{22}&\cdots&1_{m2}\\ 
		\cdots & \cdots & \cdots & \cdots \\
		1_{1m}&1_{2m}&\cdots&1_{mm}
		\end{pmatrix}\times
		\begin{pmatrix}	
		x_{11}&x_{21}&\cdots&x_{n1}\\ 
		x_{12}&x_{22}&\cdots&x_{n2}\\ 
		\cdots & \cdots & \cdots & \cdots \\
		x_{1m}&x_{2m}&\cdots&x_{nm}
		\end{pmatrix} \\ &  =   \begin{pmatrix}	
		\displaystyle\sum_{i=1}^{m}x_{1 i}&\displaystyle\sum_{i=1}^{m}x_{2 i}&\cdots&\displaystyle\sum_{i=1}^{m}x_{n i}\\ 
		\displaystyle\sum_{i=1}^{m}x_{1 i}&\displaystyle\sum_{i=1}^{m}x_{2 i}&\cdots&\displaystyle\sum_{i=1}^{m}x_{n i}\\ 
		\cdots & \cdots & \cdots & \cdots \\
		\displaystyle\sum_{i=1}^{m}x_{1 i}&\displaystyle\sum_{i=1}^{m}x_{2 i}&\cdots&\displaystyle\sum_{i=1}^{m}x_{n i}
		\end{pmatrix}
\end{aligned}
\end{split}
\end{equation}


		\item  $E 2=\left(\mathrm{X}-\frac{1}{M} E 1\right)^{T}\left(\mathrm{X}-\frac{1}{M} E 1\right)$\\\\\textbf{Respuesta:}\\
		{\scriptsize
\begin{equation}
\begin{split}
\begin{aligned}
\notag
		\left(\frac{1}{m} E 1\right) =   \begin{pmatrix}	
		\frac{1}{m} \displaystyle\sum_{i=1}^{m}x_{1 i}&\frac{1}{m} \displaystyle\sum_{i=1}^{m}x_{2 i}&\cdots&\frac{1}{m} \displaystyle\sum_{i=1}^{m}x_{n i}\\ 
		\frac{1}{m} \displaystyle\sum_{i=1}^{m}x_{1 i}&\frac{1}{m} \displaystyle\sum_{i=1}^{m}x_{2 i}&\cdots&\frac{1}{m} \displaystyle\sum_{i=1}^{m}x_{n i}\\ 
		\cdots & \cdots & \cdots & \cdots \\
		\frac{1}{m} \displaystyle\sum_{i=1}^{m}x_{1 i}&\frac{1}{m} \displaystyle\sum_{i=1}^{m}x_{2 i}&\cdots&\frac{1}{m} \displaystyle\sum_{i=1}^{m}x_{n i}
		\end{pmatrix} = \left( \begin{array}{cccc}{\overline{x}_{1}} & {\overline{x}_{2}} & {\cdots} & {\overline{x}_{N}} \\ {\overline{x}_{1}} & {\overline{x}_{2}} & {\cdots} & {\overline{x}_{N}} \\ {\cdots} & {\cdots} & {\ldots} & {\cdots} \\ {\overline{x}_{1}} & {\overline{x}_{2}} & {\cdots} & {\overline{x}_{N}}\end{array}\right)
\end{aligned}
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\begin{aligned}
\notag
		\left(X-\frac{1}{m} E 1\right) &= \begin{pmatrix}	
		x_{11}&x_{21}&\cdots&x_{n1}\\ 
		x_{12}&x_{22}&\cdots&x_{n2}\\ 
		\cdots & \cdots & \cdots & \cdots \\
		x_{1m}&x_{2m}&\cdots&x_{nm}
		\end{pmatrix} - \left( \begin{array}{cccc}{\overline{x}_{1}} & {\overline{x}_{2}} & {\cdots} & {\overline{x}_{n}} \\ {\overline{x}_{1}} & {\overline{x}_{2}} & {\cdots} & {\overline{x}_{n}} \\ {\cdots} & {\cdots} & {\ldots} & {\dots} \\ {\overline{x}_{1}} & {\overline{x}_{2}} & {\cdots} & {\overline{x}_{n}}\end{array}\right) 
		=\left( \begin{array}{cccc}{x_{11}-\overline{x}_{1}} & {x_{21}-\overline{x}_{2}} & {\cdots} & {x_{n 1}-\overline{x}_{n}} \\ {x_{12}-\overline{x}_{1}} & {x_{22}-\overline{x}_{2}} & {\cdots} & {x_{m 2}-\overline{x}_{n}} \\ {\cdots} & {\cdots} & {\cdots} & {\cdots} \\ {x_{1 m}-\overline{x}_{1}} & {x_{2 m}-\overline{x}_{2}} & {\cdots} & {x_{n m}-\overline{x}_{n}}\end{array}\right)
\end{aligned}
\end{split}
\end{equation}
\begin{equation}
\begin{split}
\begin{aligned}
\notag
		\left(X-\frac{1}{m} E 1\right)^T = \left( \begin{array}{cccc}{
		x_{11}-\overline{x}_{1}} & {x_{12}-\overline{x}_{1}} & {\cdots} & {x_{1 m}-\overline{x}_{1}} \\ 
		{x_{21}-\overline{x}_{2}} & {x_{22}-\overline{x}_{2}} & {\cdots} & {x_{2 m}-\overline{x}_{2}} \\ 
		{\cdots} & {\cdots} & {\cdots} & {\cdots} \\ 
		{x_{n 1}-\overline{x}_{n}} & {x_{n 2}-\overline{x}_{n}} & {\cdots} & {x_{n m}-\overline{x}_{n}}\end{array}\right)
		\end{aligned}
		\end{split}
		\end{equation}
		

\begin{equation}
\begin{aligned}
\notag
	&	E2 = \left( \begin{array}{cccc}{
			x_{11}-\overline{x}_{1}} & {x_{12}-\overline{x}_{1}} & {\cdots} & {x_{1 m}-\overline{x}_{1}} \\ 
		{x_{21}-\overline{x}_{2}} & {x_{22}-\overline{x}_{2}} & {\cdots} & {x_{2 m}-\overline{x}_{2}} \\ 
		{\cdots} & {\cdots} & {\cdots} & {\cdots} \\ 
		{x_{n 1}-\overline{x}_{n}} & {x_{n 2}-\overline{x}_{n}} & {\cdots} & {x_{n m}-\overline{x}_{n}}\end{array}\right) \left( \begin{array}{cccc}{x_{11}-\overline{x}_{1}} & {x_{21}-\overline{x}_{2}} & {\cdots} & {x_{n 1}-\overline{x}_{n}} \\ {x_{12}-\overline{x}_{1}} & {x_{22}-\overline{x}_{2}} & {\cdots} & {x_{m 2}-\overline{x}_{n}} \\ {\cdots} & {\cdots} & {\cdots} & {\cdots} \\ {x_{1 m}-\overline{x}_{1}} & {x_{2 m}-\overline{x}_{2}} & {\cdots} & {x_{n m}-\overline{x}_{n}}\end{array}\right)\\ 
		&=m \begin{pmatrix}	
		\frac{1}{m} \displaystyle\sum_{i=1}^{m}	(x_{1i}-\overline{x}_{1}	)(x_{1i}-\overline{x}_{1}	)&\frac{1}{m} \displaystyle\sum_{i=1}^{m}(x_{1i}-\overline{x}_{1}	)(x_{2i}-\overline{x}_{2}	)&\cdots&\frac{1}{m} \displaystyle\sum_{i=1}^{m}\left(x_{1 i}-\overline{x}_{1}\right)\left(x_{n i}-\overline{x}_{n}\right)\\ 
		\frac{1}{m} \displaystyle\sum_{i=1}^{m}(x_{1i}-\overline{x}_{1}	)(x_{2i}-\overline{x}_{2})&\frac{1}{m} \displaystyle\sum_{i=1}^{m}(x_{2i}-\overline{x}_{2}	)(x_{2i}-\overline{x}_{2}	)&\cdots&\frac{1}{m} \displaystyle\sum_{i=1}^{m}\left(x_{2 i}-\overline{x}_{2}\right)\left(x_{n i}-\overline{x}_{m}\right)\\ 
		\cdots & \cdots & \cdots & \cdots \\
		\frac{1}{m} \displaystyle\sum_{i=1}^{m}\left(x_{n i}-\overline{x}_{n}\right)\left(x_{1 i}-\overline{x}_{1}\right)&\frac{1}{m} \displaystyle\sum_{i=1}^{m}\left(x_{n i}-\overline{x}_{n}\right)\left(x_{2 i}-\overline{x}_{2}\right)&\cdots&\frac{1}{m} \displaystyle\sum_{i=1}^{m}\left(x_{n i}-\overline{x}_{n}\right)\left(x_{n i}-\overline{x}_{n}\right)\end{pmatrix}\\ &=
		m\left( \begin{array}{cccc}{\operatorname{cov}\left(\mathbf{x}_{1}, \mathbf{x}_{1}\right)} & {\operatorname{cov}\left(\mathbf{x}_{1}, \mathbf{x}_{2}\right)} & {\cdots} & {\operatorname{cov}\left(\mathbf{x}_{1}, \mathbf{x}_{n}\right)} \\ {\operatorname{cov}\left(\mathbf{x}_{2}, \mathbf{x}_{1}\right)} & {\operatorname{cov}\left(\mathbf{x}_{2}, \mathbf{x}_{2}\right)} & {\cdots} & {\operatorname{cov}\left(\mathbf{x}_{2}, \mathbf{x}_{n}\right)} \\ {\operatorname{cov}\left(\mathbf{x}_{n}, \mathbf{x}_{1}\right)} & {\operatorname{cov}\left(\mathbf{x}_{n}, \mathbf{x}_{2}\right)} & {\cdots} & {\operatorname{cov}\left(\mathbf{x}_{n}, \mathbf{x}_{n}\right)}\end{array}\right) \\ &= m\cdot \operatorname{cov}(X)
\end{aligned}
\end{equation}}
$$
E2=m\cdot \operatorname{cov}(X)
$$
	\end{enumerate}
	\item Considerar la matriz \textbf{hat} definida en regresión, $\hat{\mathrm{H}}=\mathrm{X}\left(\mathrm{X}^{\mathrm{T}} \mathrm{X}\right)^{-1} \mathrm{X}^{\mathrm{T}}$, donde $X$ es la matriz de observaciones de dimensión $N \times(d+1), \mathrm{y} \mathrm{X}^{\mathrm{T}} \mathrm{X}$, y  $X^TX$ es invertible.
	\begin{enumerate}
		\item ¿Que representa la matriz $\hat{\mathrm{H}}$ en un modelo de regresión?
		\item Identifique la propiedad mas relevante de dicha matriz en relación con regresión lineal.
	\end{enumerate}
	\item  La regla de adaptación de los pesos del Perceptron $\left(\mathbf{w}_{n e w}=\mathbf{w}_{o l d}+y \mathbf{x}\right)$ tiene la interesante propiedad de que mueve el vector de pesos en la dirección adecuada para clasificar $\textbf{x}$ de
	forma correcta. Suponga el vector de pesos $\textbf{w}$ de un modelo y un dato $\textbf{x(t)}$ mal clasificado
	respecto de dicho modelo. Probar matemáticamente que el movimiento de la regla de
	adaptación de pesos siempre produce un movimiento de $\textbf{w}$ en la dirección correcta para
	clasificar bien $\textbf{x(t)}$.
	\item Sea un problema probabilistico de clasificación binaria con etiquetas $\{0,1\}$, es decir $\mathrm{P}(\mathrm{Y}=1)=h(\mathrm{x}) \mathrm{y} \mathrm{P}(\mathrm{Y}=0)=1-h(\mathrm{x})$, para una funcion $h()$ dependiente de la muestra.
	\begin{enumerate}
		\item  Considere  una muestra i.i.d. de tamaño $\mathrm{N}\left(\mathrm{x}_{1}, \cdots, \mathrm{x}_{N}\right)$. Mostrar que la funcion $h$ que maximiza la verosimilitud de la muestra es la misma que minimiza
		$$E_{\mathrm{in}}(\mathbf{w})=\sum_{n=1}^{N}\left[y_{n}=1\right] \ln \frac{1}{h\left(\mathbf{x}_{n}\right)}+\left[y_{n}=0\right] \ln \frac{1}{1-h\left(\mathbf{x}_{n}\right)}$$ donde $[\cdot]$ vale 1 o 0 según que sea verdad o falso respectivamente la expresión en su interior.\\\\\textbf{Respuesta: }\\
		Partiéremos de la función \textbf{likelihood} para ir transformándola hasta donde queremos llegar.
		\begin{equation}
		\begin{aligned}
		\begin{split}
		\notag
			L(w) &=\prod_{i=1}^{N} P\left(y_{i} | \mathbf{x}_{i}\right) &= \prod_{i=1}^{N} \left[y_{n}=1\right] h\left(\mathbf{x}_{n}\right)+ \left[y_{n}=0\right] 1-h\left(\mathbf{x}_{n}\right)
		\end{split}
		\end{aligned}
		\end{equation}
		Ahora aplico le aplico el menos logaritmo para cambiar el productorio por un sumatorio y ademas se queden la inversa de $h(x)$ y $1-h(x)$.
				\begin{equation}
		\begin{aligned}
		\begin{split}
		\notag
		 &-\ln(\prod_{i=1}^{N} \left[y_{n}=1\right] h\left(\mathbf{x}_{n}\right)  +  \left[y_{n}=0\right] 1-h\left(\mathbf{x}_{n}\right)) \\ &= \sum_{i=1}^{N} \left[y_{n}=1\right] -\ln(h\left(\mathbf{x}_{n}\right)) +  \left[y_{n}=0\right] -\ln(1-h\left(\mathbf{x}_{n}\right)) \\ 
		 &= \sum_{i=1}^{N} \left[y_{n}=1\right] \ln(\frac{1}{h\left(\mathbf{x}_{n}\right)}) +  \left[y_{n}=0\right] \ln(\frac{1}{1-h\left(\mathbf{x}_{n}\right)})
		\end{split}
		\end{aligned}
		\end{equation}
		\item Para el caso $h(x)=\sigma\left(\mathbf{w}^{T} \mathbf{x}\right)$ mostrar que minimizar el error de la muestra en el apartado anterior es equivalente a minimizar el error muestral. 
		$$E_{\mathrm{in}}(\mathbf{w})=\frac{1}{N} \sum_{n=1}^{N} \ln \left(1+e^{-y_{n} \mathbf{w}^{T} \mathbf{x}_{n}}\right)$$\\
		\textbf{Respuesta: }\\
						\begin{equation}
		\begin{aligned}
		\begin{split}
		\notag
		E_{\mathrm{in}}(\mathbf{w}) &=\sum_{n=1}^{N}\left[y_{n}=1\right] \ln \frac{1}{\sigma\left(\mathbf{w}^{T} \mathbf{x}\right))}+\left[y_{n}=0\right] \ln \frac{1}{1-\sigma\left(\mathbf{w}^{T} \mathbf{x}\right))}\\ &=
		\sum_{n=1}^{N}\left[y_{n}=1\right] \ln \left(1+e^{\mathbf{w}^{T} \mathbf{x}_{n}}\right)-\mathbf{w}^{T} \mathbf{x}_{n}+\left[y_{n}=0\right] \ln \left(1+e^{\mathbf{w}^{T} \mathbf{x}_{n}}\right)\\ &=
-\sum_{n=1}^{N}\left[y_{n}=1\right]\mathbf{w}^{T} \mathbf{x}_{n}+   \underbrace{\sum_{n=1}^{N}\left[y_{n}=0\right] \ln \left(1+e^{\mathbf{w}^{T} \mathbf{x}_{n}}\right)}
		\end{split}
		\end{aligned}
		\end{equation}
	\end{enumerate}
\item Derivar el error $E_{in}$ para mostrar que en regresión logística se verifica: $$\nabla E_{\mathrm{in}}(\mathbf{w})=-\frac{1}{N} \sum_{n=1}^{N} \frac{y_{n} \mathbf{x}_{n}}{1+e^{y_{n} \mathbf{w}^{T} \mathbf{x}_{n}}}=\frac{1}{N} \sum_{n=1}^{N}-y_{n} \mathbf{x}_{n} \sigma\left(-y_{n} \mathbf{w}^{T} \mathbf{x}_{n}\right)$$
Argumentar sobre si un ejemplo mal clasificado contribuye al gradiente más que un ejemplo bien clasificado.
\item Definamos el error en un punto $(x_n,y_n)$ por $$\mathbf{e}_{n}(\mathbf{w})=\max \left(0,-y_{n} \mathbf{w}^{T} \mathbf{x}_{n}\right)$$ Argumentar si con esta función de error el algoritmo $PLA$ puede interpretarse como SGD sobre $e_n$ con tasa de aprendizaje $v=1$.
\end{enumerate}

\end{document}
